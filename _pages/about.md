---
permalink: /
title: "ğŸ‘‹ Hello there! I'm Yash"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

![Illustration of combining vision and language modalities](/images/multi_modal_ai.png){: .align-right width="300px"}

ğŸ‘¨ğŸ»â€ğŸ“ Iâ€™m a masterâ€™s student in **Trustworthy & Responsible AI (TRAI)** at **[Ã‰cole Polytechnique](https://www.polytechnique.edu/en)**

ğŸ”¬ My research interests are at the intersection of **multimodal AI** (vision, audio and language) and **reliable/efficient LLM & VLM systems**. 

ğŸ“š I am currently collaborating with Georgia Techâ€™s **[Financial Services Innovation Lab](https://qcf.gatech.edu/partner)** on problems in multimodal finance.

ğŸ” **Actively seeking research internships (Spring/Summer 2026)** in **multimodal learning**, **alignment**, and **training LLMs/VLMs** - Europe (FR/EU) preferred, open worldwide.
> 
> I love end-to-end work: data â†’ modeling â†’ eval â†’ lightweight demos.

### What Iâ€™m focused on now
- ğŸ¥ **Multimodal learning & generation:** Videoâ€“audioâ€“text fusion, long-range temporal reasoning, controllable generation.
- ğŸ‘ï¸ **Computer vision:** Video understanding, detection/segmentation, retrieval, vision-language grounding.
- ğŸ§© **Alignment & reliability:** Preference/contrastive learning, eval pipelines, robustness/bias checks, model monitoring.
- âš™ï¸ **Systems & efficiency:** Memory/latency-aware inference, reproducible data/workflows, simple productizable demos.

# Selected Highlights

## Publications
- **[VideoConviction (KDD 2025)](https://doi.org/10.1145/3711896.3737417):**  
  Introduced the first expert-annotated **multimodal finance benchmark**, capturing *conviction* in stock market recommendations from YouTube finfluencers.  
  - 6,000+ annotations across 288 videos (43 hrs), 457 annotation hours.  
  - Benchmarks LLMs and MLLMs on ticker/action/conviction extraction.  
  - Portfolio analysis shows inverse strategies (betting against finfluencers) outperform S&P 500 but with higher risk.  
  - Dataset + code released on [GitHub](https://github.com/gtfintechlab/VideoConviction) / [Hugging Face](https://huggingface.co/datasets/gtfintechlab/VideoConviction).

- **[FinCap (ICCV 2025)](https://arxiv.org/abs/2509.25745):**  
  Multimodal captioning benchmark for financial videos.  
  - Generates dense captions across **video, audio, text, and descriptions**.  
  - Evaluates diverse model families (OpenAI, Gemini, Qwen, Phi, etc.).  
  - Uses **reference-free evaluation** (G-Eval, PAC-S) to assess informativeness and alignment.

### Background
- ğŸ“ **MSc&T Trustworthy and Responsible AI (M1)**: [Ã‰cole Polytechnique](https://www.polytechnique.edu/en) (current)  
- ğŸ“ **B.E., Computer Science**: [BITS Pilani](https://www.bits-pilani.ac.in/), India
- ğŸ§ª **Research**: [Georgia Tech](https://www.gatech.edu/) (multimodal finance), [IIIT-Delhi](https://midas.iiitd.ac.in/bio) (author profiling, citation/keyphrase gen)


### Letâ€™s collaborate
Iâ€™m especially interested in **alignment for VLMs**, **multimodal representation learning**, **evaluation of generative models**, and **efficient training/inference**.  
If youâ€™re building in these areas, Iâ€™d love to chat about internships or collaborations.